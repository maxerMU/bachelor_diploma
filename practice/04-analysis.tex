\chapter{Основная часть}

\section{Формализованная постановка задачи}
Цель работы -- разработать метод распознавания летательных аппаратов с аэрофотоснимков.

Для достижения поставленной цели требуется выполнить следующие задачи:
\begin{itemize}
	\item провести анализ существующих программных подходов распознавания летательных аппаратов с аэрофотоснимков (проведено выше);
	\item разработать метод распознавания летальной техники на аэрофотоснимках;
	\item реализовать спроектированный метод;
	\item провести исследование точности распознавания модели на тестовой выборке при различных подходах к обучению.
\end{itemize}

На вход методу подается изображение со снимком аэропорта, где находятся летательные аппараты одного из 20 классов. Результатом работы метода является распознанные на изображении самолеты и их модели. Требуемая точность работы метода на тестовом наборе данных должна составлять не менее 75 процентов.

На входное изображение накладываются следующие ограничения:
\begin{itemize}
	\item изображение сделано в дневное время суток;
	\item размер изображения 800 на 800 пикселей;
	\item размер самолетов на изображении более 70 пикселей.
\end{itemize}

На рисунке \ref{img:idef0A0} приведена IDEF-0 диаграмма уровня А0 метода распознавания летательных аппаратов с аэрофотоснимков с использованием нейронных сетей.

\includeimage
{idef0A0} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.85\textwidth} % Ширина рисунка
{IDEF-0 диаграмма метод распознавания летательных аппаратов с аэрофотоснимков} % Подпись рисунка

IDEF-0 диаграмма метода распознавания летательной техники с аэрофотоснимков уровня A1 приведена на рисунке \ref{img:idef0A1}.
\includeimage
{idef0A1} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.85\textwidth} % Ширина рисунка
{IDEF-0 диаграмма уровня A1} % Подпись рисунка

Решение задачи распознавания самолетов на аэрофотоснимках было поделено на две подзадачи: детектирование объектов на изображении и их классификация. Результатом решения задачи детектирования являются ограничительные рамки, в которых предположительно находятся летательные объекты.

Перцептрон плохо подходит для задачи распознавания объектов на изображении в силу того, что для его работы требуется предварительная обработка изображений, он не устойчив к шумам и обучается дольше, чем сверточные и капсульные нейронные сети.

Капсульные нейронные сети в отличие от перцептрона не нуждаются в предварительной обработке изображений, устойчивы к сдвигу изображений и для их обучения нужен набор данных меньшего размера. Недостатком таких сетей является их неустойчивость к шумам на изображении.

Таким образом, лучше всего для решения задачи распознавания летательных аппаратов на изображении подходят сверточные нейронные сети, которые устойчивы к сдвигу и шумам на входном снимке и требуют меньший набор данных для обучения по сравнению с капсульными сетями. 

\section{Сравнение сверточных нейронных сетей}

\subsection{AlexNet}\label{sec:alexnet}

AlexNet по своему принципу не сильно отличается от LeNet. В AlexNet используется больше сверточных слоев, а в слоях субдисретизации используется Max Pooling. В качестве функции активации полносвязного слоя используется ReLU. Общая схема архитектуры сети приведена на рисунке \ref{img:alexnet}.

\includeimage
{alexnet} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.6\textwidth} % Ширина рисунка
{Архитектура AlexNet} % Подпись рисунка

Преимуществом данной архитектуры является высокая точность распознавания -- в 2012 AlexNet показала рекордный результат в точности распознания 1000 различных объектов в соревновании ImageNet.

Главным недостатком является большое число параметров, использующихся при обучении, -- около 60 миллионов \cite{architectures}. Такое количество параметров требует значительно большие вычислительные мощности и память в сравнении с LeNet. Также потребуется больше элементов в обучающей выборке, чтобы корректно настроить эти параметры.

\subsection{GoogLeNet}

В GoogLeNet было введено понятие Inception блока, формальное представление которого приведено на рисунке \ref{img:inception}.

\includeimage
{inception} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{H} % Положение рисунка (см. figure из пакета float)
{0.85\textwidth} % Ширина рисунка
{Формальное представление Inception блока} % Подпись рисунка

В таком блоке выходы с предыдущего слоя параллельно обрабатываются сверткой 1 на 1, блоком из сверток 1 на 1 и 3 на 3, блоком из сверток 1 на 1 и 5 на 5, блоком из Max Pooling размером 3 на 3 и свертки 1 на 1. Далее выходы со всех этих блоков объединяются и передаются дальше.

Фильтр 1 на 1 используется для уменьшения количества параметров в сверточных слоях и ускорения вычислений. Он позволяет сократить количество каналов входного изображения до более низкого значения, что уменьшает количество вычислений при применении более крупных фильтров. Кроме того, фильтры 1 на 1 могут использоваться для комбинирования признаков разных каналов, что улучшает качество классификации \cite{googlenet}.

Такой подход имеет два основных преимущества \cite{googlenet}:
\begin{itemize}
	\item возможность увеличения количества блоков на каждом этапе без неконтролируемого роста вычислительной сложности;
	\item визуальная информация обрабатывается в различных масштабах, а затем агрегируется, чтобы на следующем этапе можно было абстрагировать признаки из разных масштабов одновременно.
\end{itemize}

Использование таких блоков в архитектуре GoogLeNet позволяет сократить число параметров примерно в 12 раз по сравнению с AlexNet, при этом точность классификации сети не падает \cite{googlenet}.

Для борьбы с затуханием градиента в GoogLeNet используется следующий прием: помимо классификатора в конце нейронной сети добавляется еще один после третьего Inception блока и еще один после 6. Во время обучения функция потерь считается не только по значениям из последнего классификатора, но и по 2 добавленным, домжноженным на некоторый коэффициент. Итоговое выражение для подсчета функции потерь определяется следующей зависимостью \ref{formula:gooleloss}
\begin{equation}\label{formula:gooleloss}
T\!L = l + k * (l_{\rm{1}} + l_{\rm{2}}),
\end{equation}
где $T\!L$ -- общее значение функции потерь, которое нужное уменьшать во время обучения, $l$ -- значение функции потерь, посчитанное по выходам из последнего классификатора, $k$ -- некоторый коэффициент значимости, на который домножаются значения функции потерь, посчитанные по выходам двух других классификаторов, в GoogLeNet этот коэффициент равен 0.3, $l_1$ и $l_2$ -- это значения функции потерь посчитанные по выходам классификаторов, добавленный в начале и в середине соответственно.


\subsection{SimpLeNet}
Архитектура нейронной сети SimpLeNet состоит из сверточных слоев только размером 3 на 3 пикселя. Для уменьшения размеров изображения используются слои пуллинга с ядром размера 2 на 2 пикселя и шагом два пикселя. После каждого из сверточных слоев применяется слой нормализации и функция активации ReLU.

Благодаря использованию сверток только размером 3 на 3, в такой нейронной сети меньше обучаемых параметров по сравнению с сетями, где используются свертки больше размеров. Согласно исследованиям \cite{mobilenets} в такой сети используется в два с половиной раза меньше вычислительных операций по сравнению с GoogLeNet и в 15 раз меньше обучаемых параметров по сравнению с AlexNet.

Использование меньшего числа параметров приводит к уменьшению скорости обучения нейронной сети, а также к меньшим ограничениям на вычислительные ресурсы при использовании модели, что важно, так как все изображения, которые поступают на вход модели, сделаны с беспилотных летательных аппаратов и могут на них же обрабатываться.

\subsection{Yolo}
%TODO ссылка на йоло
Нейронная сеть Yolo предназначена для детектирования объектов на изображении. Результатом работы такой сети являются граничные области найденных объектов. В отличие от других сетей Yolo способна детектировать сразу несколько объектов на изображении.

Алгоритм работы yolo состоит из двух частей: сверточной нейронной сети и последующей обработки ее результатов. Сверточная нейронная сеть должна выделить ограничивающие рамки объектов на изображении. Задача последующей обработки заключается в том, чтобы провести пороговую фильтрацию полученных из сверточной сети ограничивающий рамок на основе вероятности нахождения в них объекта, а также исключить из рассмотрения схожие рамки.

Сверточная нейронная сеть разбивает входное изображения на ячейки некоторого размера и для каждой из них предсказывает следующий набор параметров:
\begin{itemize}
	\item $p$ -- вероятность нахождения центра какого-нибудь объекта в этой ячейке, принимает значения в диапазоне от нуля до одного;
	\item $x$ -- центр объекта по оси абсцисс внутри этой ячейки, левый верхний угол имеет координаты 0 и 0, правый нижний -- 1 и 1, соответственно значение $x$ находится в диапазоне от нуля до одного;
	\item $y$ -- центр объекта по оси ординат;
	\item $w$ -- ширина ограничительной рамки, поделенная на ширину ячейки, принимает значения больше нуля;
	\item $h$ -- высота ограничительной рамки, поделенная на высоту ячейки, принимает значения больше нуля;
	\item $c_i$ -- вероятность того, что в рамке находится объект i-го класса.
\end{itemize}

Таким образом, выход из сверточной нейронной сети имеет размерность $C\rm{x}C\rm{x}(5+classes)$, где $C$ -- число размер сетки по высоте и ширине, $classes$ -- число классов, которые распознает нейронная сеть.

Координаты центра по оси абсцисс и ординат ограничительной рамки получаются из формул \ref{formula:yolox} и \ref{formula:yoloy} соответственно
\begin{equation}\label{formula:yolox}
x = \sigma(t_{\rm{x}}) + c_{\rm{x}},
\end{equation}
\begin{equation}\label{formula:yoloy}
y = \sigma(t_{\rm{y}}) + c_{\rm{y}},
\end{equation}
где $x$ и $y$ -- координаты центра рамка, $c_{\rm{x}}$ и $c_{\rm{y}}$ -- координаты левого верхнего угла ячейки, в которой находится центр рамки, $t_{\rm{x}}$ и $t_{\rm{y}}$ -- выходы из нейронной сети.

Высота и ширина ограничительной рамки определяются по формулам \ref{formula:yoloh} и \ref{formula:yolow} соответственно
\begin{equation}\label{formula:yoloh}
h =  p_{\rm{h}}e^{t_{\rm{h}}},
\end{equation}
\begin{equation}\label{formula:yolow}
w =  p_{\rm{w}}e^{t_{\rm{w}}},
\end{equation}
где $t_{\rm{h}}$ и $t_{\rm{w}}$ -- выходы из нейронной сети, $p_{\rm{h}}$ и $p_{\rm{w}}$ -- высота и ширина якоря, который используются для определения, какие ограничивающие рамки должны быть выданы алгоритмом для конкретных объектов на изображении.

В нейронной сети Yolo v3 входное изображение разделяется на сетка трех разных масштабов и для каждой ячейки используется по три якоря.

Для удаления схожих рамок на этапе обработки выходов сверточной сети используется алгоритм non maximum supperssion, суть работы которого заключается в следующем:
\begin{itemize}
	\item ограничивающие рамки сортируются по вероятности нахождения в них объекта;
	\item выбирается рамка с самой высокой вероятностью и она сохраняется в окончательном списке ограничивающих рамок;
	\item после этого отбрасываются все рамки, которые нашли объект того же класса и имеют значение перекрытия с выбранной на предыдущем шаге рамкой больше порогового;
	\item процесс продолжается, пока не будут обработаны все оставшиеся рамки.
\end{itemize}

Перекрытие двух рамок высчитывается делением площади их пересечения на площадь их объединения.


\subsection{Вывод}
Архитектура CapsNet плохо подходит для решения задачи распознавания летательных аппаратов с аэрофотоснимков, так как не устойчива к шумам во входном изображении.

Архитектура LeNet менее глубокая по сравнению с другими архитектурами, что ограничивает ее способность к достижению большей точности классификации.

Нейронные сети AlexNet и GoogLeNet плохо подходят для решения поставленной задачи в силу большего числа обучающих параметров по сравнению с другими архитектурами, так как большее число параметров приводит к большим ограничениям на вычислительные ресурсы, увеличению времени обучения и требуемого размера обучающей выборки.

Таким образом, для решения задачи классификации лучше всего подходит архитектура SimpLeNet за счет использования меньшего числа параметров по сравнению с другими архитектурами, а также за счет использования слоев нормализации, которые ускоряют процесс обучения и повышают точность модели, а для решения задачи детектирования -- Yolo.